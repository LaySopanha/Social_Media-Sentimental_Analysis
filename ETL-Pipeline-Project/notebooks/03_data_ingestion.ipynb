{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark version: 3.3.2\n"
     ]
    }
   ],
   "source": [
    "# On your Windows client (VSCode)\n",
    "import pyspark\n",
    "print(\"PySpark version:\", pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_HOME: C:\\spark-3.3.2-bin-hadoop3\n",
      "HADOOP_HOME: C:\\hadoop-3.4.1\n",
      "JAVA_HOME: C:\\jdk-11.0.26.4-hotspot\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\\\spark-3.3.2-bin-hadoop3\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop-3.4.1\"\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\jdk-11.0.26.4-hotspot\"\n",
    "os.environ[\"PATH\"] += f\";{os.environ['SPARK_HOME']}\\\\bin;{os.environ['HADOOP_HOME']}\\\\bin\"\n",
    "\n",
    "print(\"SPARK_HOME:\", os.environ[\"SPARK_HOME\"])\n",
    "print(\"HADOOP_HOME:\", os.environ[\"HADOOP_HOME\"])\n",
    "print(\"JAVA_HOME:\", os.environ[\"JAVA_HOME\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark Session Created Successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Adjust SparkSession configuration to match your working setup\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://hadoop-master-node:7077\") \\\n",
    "    .appName(\"SentimentDemo\") \\\n",
    "    .config(\"spark.driver.memory\", \"512M\") \\\n",
    "    .config(\"spark.executor.memory\", \"512M\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.driver.host\", \"192.168.1.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark Session Created Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process kaggle data to hadoop using spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/twitter-kaggle-sentiment-cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 1.5.3\n",
      "PySpark version: 3.3.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"PySpark version:\", pyspark.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target           int64\n",
      "id               int64\n",
      "date            object\n",
      "flag            object\n",
      "user            object\n",
      "text            object\n",
      "cleaned_text    object\n",
      "sentiment        int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Download\\Anaconda\\anaconda3\\envs\\sentiment-analysis-env\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType\n",
    "\n",
    "# Modify schema to use LongType for 'id' field\n",
    "schema = StructType([\n",
    "    StructField(\"target\", IntegerType(), True),\n",
    "    StructField(\"id\", LongType(), True),  # Use LongType here for larger numbers\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"flag\", StringType(), True),\n",
    "    StructField(\"user\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"cleaned_text\", StringType(), True),\n",
    "    StructField(\"sentiment\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create a Spark DataFrame from the cleaned Pandas DataFrame\n",
    "spark_df = spark.createDataFrame(df, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the Spark DataFrame to HDFS with the header\n",
    "spark_df.write.mode(\"append\").option(\"header\", \"true\").csv(\"hdfs://hadoop-master-node:9000/sentiment/processed/twitter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+---------+\n",
      "|target|        id|                date|    flag|           user|                text|        cleaned_text|sentiment|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+---------+\n",
      "|     0|2200003196|Tue Jun 16 18:18:...|NO_QUERY|LaLaLindsey0609|@chrishasboobs AH...|        ahhh hope ok|       -1|\n",
      "|     0|1467998485|Mon Apr 06 23:11:...|NO_QUERY|    sexygrneyes|@misstoriblack co...| cool tweet app razr|       -1|\n",
      "|     0|2300048954|Tue Jun 23 13:40:...|NO_QUERY|     sammydearr|@TiannaChaos i kn...|know famili drama...|       -1|\n",
      "|     0|1993474027|Mon Jun 01 10:26:...|NO_QUERY|    Lamb_Leanne|School email won'...|school email wont...|       -1|\n",
      "|     0|2256550904|Sat Jun 20 12:56:...|NO_QUERY|    yogicerdito|upper airways pro...|upper airway problem|       -1|\n",
      "|     0|2052380495|Sat Jun 06 00:32:...|NO_QUERY|      Yengching|Going to miss Pas...|go miss pastor se...|       -1|\n",
      "|     4|1983449090|Sun May 31 13:10:...|NO_QUERY|       jessig06|on lunch....dj sh...|    lunchdj come eat|        1|\n",
      "|     0|2245479748|Fri Jun 19 16:11:...|NO_QUERY| felicityfuller|@piginthepoke oh ...|        oh feel like|       -1|\n",
      "|     0|1770705699|Mon May 11 22:01:...|NO_QUERY|    stephiiheyy|gahh noo!peyton n...|gahh noopeyton ne...|       -1|\n",
      "|     4|1970386589|Sat May 30 03:39:...|NO_QUERY|      wyndwitch|@mrstessyman than...|thank glad like p...|        1|\n",
      "|     4|2052206835|Fri Jun 05 23:59:...|NO_QUERY|       salmafan|@PerezHilton Zach...|zach make pee sit...|        1|\n",
      "|     4|1992273171|Mon Jun 01 08:29:...|NO_QUERY|     spanishman|to sum up my day ...|sum day one word ...|        1|\n",
      "|     4|2069921255|Sun Jun 07 16:46:...|NO_QUERY|   DannyMacRant|@k9wkj Great mind...|great mind think ...|        1|\n",
      "|     0|2204735567|Wed Jun 17 03:00:...|NO_QUERY|   Arabellakind|Is Poorly and in ...|          poorli bed|       -1|\n",
      "|     4|1553585468|Sat Apr 18 14:39:...|NO_QUERY|         Reisei|@LilPecan Oh, tha...|oh that realli gr...|        1|\n",
      "|     4|1834598659|Mon May 18 03:32:...|NO_QUERY|     andylackie|@wizely lol, calm...|lol calm got day ...|        1|\n",
      "|     4|2190762188|Tue Jun 16 03:39:...|NO_QUERY|     biaromerok|i'm feeling quite...|im feel quit slee...|        1|\n",
      "|     0|1973926253|Sat May 30 12:06:...|NO_QUERY|         risha_|@nadalnews Yeah  ...|yeah mathieu tota...|       -1|\n",
      "|     0|2003334341|Tue Jun 02 06:20:...|NO_QUERY|  stephrunscity|ugh, morning's of...|ugh morn rough start|       -1|\n",
      "|     0|2063670257|Sun Jun 07 03:36:...|NO_QUERY|    cathymcewan|  just bit my tongue|           bit tongu|       -1|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV files from HDFS, specifying that the first row contains the header\n",
    "df = spark.read.csv(\"hdfs://hadoop-master-node:9000/sentiment/processed/twitter\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the DataFrame to check the result\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id = '-_LwwR2GAkBuc-mvHUVSEQ',\n",
    "    client_secret = '5X25s7-NfqlEnlstXzdvHwWEKyyMfw',\n",
    "    user_agent = 'Social Media Sentiment Analysis v1.0 by /u/Massive_Strategy75'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_posts():\n",
    "    posts = reddit.subreddit('WorldNews').search('world', limit = 100)\n",
    "    # post_list = list(posts)\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_news_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = fetch_posts()\n",
    "for post in posts:\n",
    "    world_news_data.append({\n",
    "        \"title\" : post.title,\n",
    "        \"score\": post.score,\n",
    "        \"create_at\": post.created_utc,\n",
    "        \"url\" : post.url,\n",
    "        \"num_comments\" : post.num_comments,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_world_news = pd.DataFrame(world_news_data)\n",
    "df_world_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment-analysis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
